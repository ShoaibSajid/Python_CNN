{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from yolov2tiny import Yolov2\n",
    "import eecs598\n",
    "from cnn_scratch import DeepConvNet, Conv, ConvB, Conv_ReLU, Conv_ReLU_Pool, Conv_BatchNorm_ReLU, Conv_BatchNorm_ReLU_Pool\n",
    "from cnn_torch import FastConv, FastConvWB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch = DeepConvNet(input_dims=(3, 416, 416),\n",
    "                        num_filters=[16, 32, 64, 128, 256, 512, 1024, 1024],\n",
    "                        max_pools=[0, 1, 2, 3, 4],\n",
    "                        weight_scale='kaiming',\n",
    "                        batchnorm=True,\n",
    "                        dtype=torch.float32, device='cpu')\n",
    "\n",
    "class WeightLoader(object):\n",
    "    def __init__(self):\n",
    "        super(WeightLoader, self).__init__()\n",
    "        self.start = 0\n",
    "        self.buf = None\n",
    "        self.b = 'b'\n",
    "        self.g = 'g'\n",
    "        self.rm = 'rm'\n",
    "        self.rv = 'rv'\n",
    "        \n",
    "    def load_conv_bn(self, conv_model, bn_model):\n",
    "\n",
    "        num_w = conv_model.weight.numel()\n",
    "        num_b = bn_model.bias.numel()\n",
    "\n",
    "        bn_model.bias.data.copy_(\n",
    "            torch.reshape(torch.from_numpy(self.buf[self.start:self.start + num_b]), bn_model.bias.size()))\n",
    "\n",
    "        if bn_model.bias.data.shape == scratch.params['beta0'].shape:\n",
    "            scratch.params['beta0'] = bn_model.bias.data\n",
    "            with open('./weight_parameter/bn_param/bias/{}'.format(0), mode='w') as f:\n",
    "                f.write(str(bn_model.bias.data))\n",
    "        elif bn_model.bias.data.shape == scratch.params['beta1'].shape:\n",
    "            scratch.params['beta1'] = bn_model.bias.data\n",
    "            with open('./weight_parameter/bn_param/bias/{}'.format(1), mode='w') as f:\n",
    "                f.write(str(bn_model.bias.data))\n",
    "        elif bn_model.bias.data.shape == scratch.params['beta2'].shape:\n",
    "            scratch.params['beta2'] = bn_model.bias.data\n",
    "            with open('./weight_parameter/bn_param/bias/{}'.format(2), mode='w') as f:\n",
    "                f.write(str(bn_model.bias.data))\n",
    "        elif bn_model.bias.data.shape == scratch.params['beta3'].shape:\n",
    "            scratch.params['beta3'] = bn_model.bias.data\n",
    "            with open('./weight_parameter/bn_param/bias/{}'.format(3), mode='w') as f:\n",
    "                f.write(str(bn_model.bias.data))\n",
    "        elif bn_model.bias.data.shape == scratch.params['beta4'].shape:\n",
    "            scratch.params['beta4'] = bn_model.bias.data\n",
    "            with open('./weight_parameter/bn_param/bias/{}'.format(4), mode='w') as f:\n",
    "                f.write(str(bn_model.bias.data))\n",
    "        elif bn_model.bias.data.shape == scratch.params['beta5'].shape:\n",
    "            scratch.params['beta5'] = bn_model.bias.data\n",
    "            with open('./weight_parameter/bn_param/bias/{}'.format(5), mode='w') as f:\n",
    "                f.write(str(bn_model.bias.data))\n",
    "        elif (bn_model.bias.data.shape == scratch.params['beta6'].shape) and self.b == \"b\":\n",
    "            scratch.params['beta6'] = bn_model.bias.data\n",
    "            with open('./weight_parameter/bn_param/bias/{}'.format(6), mode='w') as f:\n",
    "                f.write(str(bn_model.bias.data))\n",
    "            self.b = 'bb'\n",
    "        elif (scratch.params['beta7'].shape == bn_model.bias.data.shape) and self.b == \"bb\":\n",
    "            scratch.params['beta7'] = bn_model.bias.data\n",
    "            with open('./weight_parameter/bn_param/bias/{}'.format(7), mode='w') as f:\n",
    "                f.write(str(bn_model.bias.data))\n",
    "\n",
    "        self.start = self.start + num_b\n",
    "\n",
    "        \n",
    "\n",
    "        bn_model.weight.data.copy_(\n",
    "            torch.reshape(torch.from_numpy(self.buf[self.start:self.start + num_b]), bn_model.bias.size()))\n",
    "\n",
    "\n",
    "        if bn_model.weight.data.shape == scratch.params['gamma0'].shape:\n",
    "            scratch.params['gamma0'] = bn_model.weight.data\n",
    "            with open('./weight_parameter/bn_param/gamma/{}'.format(0), mode='w') as f:\n",
    "                f.write(str(bn_model.weight.data))\n",
    "        elif bn_model.weight.data.shape == scratch.params['gamma1'].shape:\n",
    "            scratch.params['gamma1'] = bn_model.weight.data\n",
    "            with open('./weight_parameter/bn_param/gamma/{}'.format(1), mode='w') as f:\n",
    "                f.write(str(bn_model.weight.data))\n",
    "        elif bn_model.weight.data.shape == scratch.params['gamma2'].shape:\n",
    "            scratch.params['gamma2'] = bn_model.weight.data\n",
    "            with open('./weight_parameter/bn_param/gamma/{}'.format(2), mode='w') as f:\n",
    "                f.write(str(bn_model.weight.data))\n",
    "        elif bn_model.weight.data.shape == scratch.params['gamma3'].shape:\n",
    "            scratch.params['gamma3'] = bn_model.weight.data\n",
    "            with open('./weight_parameter/bn_param/gamma/{}'.format(3), mode='w') as f:\n",
    "                f.write(str(bn_model.weight.data))\n",
    "        elif bn_model.weight.data.shape == scratch.params['gamma4'].shape:\n",
    "            scratch.params['gamma4'] = bn_model.weight.data\n",
    "            with open('./weight_parameter/bn_param/gamma/{}'.format(4), mode='w') as f:\n",
    "                f.write(str(bn_model.weight.data))\n",
    "        elif bn_model.weight.data.shape == scratch.params['gamma5'].shape:\n",
    "            scratch.params['gamma5'] = bn_model.weight.data\n",
    "            with open('./weight_parameter/bn_param/gamma/{}'.format(5), mode='w') as f:\n",
    "                f.write(str(bn_model.weight.data))\n",
    "        elif (bn_model.weight.shape == scratch.params['gamma6'].shape) and self.g == \"g\":\n",
    "            scratch.params['gamma6'] = bn_model.weight.data\n",
    "            with open('./weight_parameter/bn_param/gamma/{}'.format(6), mode='w') as f:\n",
    "                f.write(str(bn_model.weight.data))\n",
    "            self.g = 'gg'\n",
    "        elif (scratch.params['gamma7'].shape == bn_model.weight.data.shape) and self.g == \"gg\":\n",
    "            scratch.params['gamma7'] = bn_model.weight.data\n",
    "            with open('./weight_parameter/bn_param/gamma/{}'.format(7), mode='w') as f:\n",
    "                f.write(str(bn_model.weight.data))\n",
    "\n",
    "        self.start = self.start + num_b\n",
    "\n",
    "        bn_model.running_mean.copy_(\n",
    "            torch.reshape(torch.from_numpy(self.buf[self.start:self.start + num_b]), bn_model.bias.size()))\n",
    "\n",
    "        if bn_model.running_mean.data.shape == scratch.bn_params[0]['running_mean'].shape:\n",
    "            scratch.bn_params[0]['running_mean'] = bn_model.running_mean.data\n",
    "            with open('./weight_parameter/bn_param/running_mean/{}'.format(0), mode='w') as f:\n",
    "                f.write(str(bn_model.running_mean.data))\n",
    "        elif bn_model.running_mean.data.shape == scratch.bn_params[1]['running_mean'].shape:\n",
    "            scratch.bn_params[1]['running_mean'] = bn_model.running_mean.data\n",
    "            with open('./weight_parameter/bn_param/running_mean/{}'.format(1), mode='w') as f:\n",
    "                f.write(str(bn_model.running_mean.data))\n",
    "        elif bn_model.running_mean.data.shape == scratch.bn_params[2]['running_mean'].shape:\n",
    "            scratch.bn_params[2]['running_mean'] = bn_model.running_mean.data\n",
    "            with open('./weight_parameter/bn_param/running_mean/{}'.format(2), mode='w') as f:\n",
    "                f.write(str(bn_model.running_mean.data))\n",
    "        elif bn_model.running_mean.data.shape == scratch.bn_params[3]['running_mean'].shape:\n",
    "            scratch.bn_params[3]['running_mean'] = bn_model.running_mean.data\n",
    "            with open('./weight_parameter/bn_param/running_mean/{}'.format(3), mode='w') as f:\n",
    "                f.write(str(bn_model.running_mean.data))\n",
    "        elif bn_model.running_mean.data.shape == scratch.bn_params[4]['running_mean'].shape:\n",
    "            scratch.bn_params[4]['running_mean'] = bn_model.running_mean.data\n",
    "            with open('./weight_parameter/bn_param/running_mean/{}'.format(4), mode='w') as f:\n",
    "                f.write(str(bn_model.running_mean.data))\n",
    "        elif bn_model.running_mean.data.shape == scratch.bn_params[5]['running_mean'].shape:\n",
    "            scratch.bn_params[5]['running_mean'] = bn_model.running_mean.data\n",
    "            with open('./weight_parameter/bn_param/running_mean/{}'.format(5), mode='w') as f:\n",
    "                f.write(str(bn_model.running_mean.data))\n",
    "        elif bn_model.running_mean.data.shape == scratch.bn_params[6]['running_mean'].shape and self.rm == \"rm\":\n",
    "            scratch.bn_params[6]['running_mean'] = bn_model.running_mean.data\n",
    "            with open('./weight_parameter/bn_param/running_mean/{}'.format(6), mode='w') as f:\n",
    "                f.write(str(bn_model.running_mean.data))\n",
    "            self.rm = \"rmrm\"\n",
    "        elif bn_model.running_mean.data.shape == scratch.bn_params[7]['running_mean'].shape and self.rm == \"rmrm\":\n",
    "            scratch.bn_params[7]['running_mean'] = bn_model.running_mean.data\n",
    "            with open('./weight_parameter/bn_param/running_mean/{}'.format(7), mode='w') as f:\n",
    "                f.write(str(bn_model.running_mean.data))\n",
    "\n",
    "        self.start = self.start + num_b\n",
    "\n",
    "        bn_model.running_var.copy_(\n",
    "            torch.reshape(torch.from_numpy(self.buf[self.start:self.start + num_b]), bn_model.bias.size()))\n",
    "\n",
    "        if bn_model.running_var.data.shape == scratch.bn_params[0]['running_var'].shape:\n",
    "            scratch.bn_params[0]['running_var'] = bn_model.running_var.data\n",
    "            with open('./weight_parameter/bn_param/running_var/{}'.format(0), mode='w') as f:\n",
    "                f.write(str(bn_model.running_var.data))\n",
    "        elif bn_model.running_var.data.shape == scratch.bn_params[1]['running_var'].shape:\n",
    "            scratch.bn_params[1]['running_var'] = bn_model.running_var.data\n",
    "            with open('./weight_parameter/bn_param/running_var/{}'.format(1), mode='w') as f:\n",
    "                f.write(str(bn_model.running_var.data))\n",
    "        elif bn_model.running_var.data.shape == scratch.bn_params[2]['running_var'].shape:\n",
    "            scratch.bn_params[2]['running_var'] = bn_model.running_var.data\n",
    "            with open('./weight_parameter/bn_param/running_var/{}'.format(2), mode='w') as f:\n",
    "                f.write(str(bn_model.running_var.data))\n",
    "        elif bn_model.running_var.data.shape == scratch.bn_params[3]['running_var'].shape:\n",
    "            scratch.bn_params[3]['running_var'] = bn_model.running_var.data\n",
    "            with open('./weight_parameter/bn_param/running_var/{}'.format(3), mode='w') as f:\n",
    "                f.write(str(bn_model.running_var.data))\n",
    "        elif bn_model.running_var.data.shape == scratch.bn_params[4]['running_var'].shape:\n",
    "            scratch.bn_params[4]['running_var'] = bn_model.running_var.data\n",
    "            with open('./weight_parameter/bn_param/running_var/{}'.format(4), mode='w') as f:\n",
    "                f.write(str(bn_model.running_var.data))\n",
    "        elif bn_model.running_var.data.shape == scratch.bn_params[5]['running_var'].shape:\n",
    "            scratch.bn_params[5]['running_var'] = bn_model.running_var.data\n",
    "            with open('./weight_parameter/bn_param/running_var/{}'.format(5), mode='w') as f:\n",
    "                f.write(str(bn_model.running_var.data))\n",
    "        elif bn_model.running_var.data.shape == scratch.bn_params[6]['running_var'].shape and self.rv == \"rv\":\n",
    "            scratch.bn_params[6]['running_var'] = bn_model.running_var.data\n",
    "            with open('./weight_parameter/bn_param/running_var/{}'.format(6), mode='w') as f:\n",
    "                f.write(str(bn_model.running_var.data))\n",
    "            self.rv = \"rvrv\"\n",
    "        elif bn_model.running_var.data.shape == scratch.bn_params[7]['running_var'].shape and self.rv == \"rvrv\":\n",
    "            scratch.bn_params[7]['running_var'] = bn_model.running_var.data\n",
    "            with open('./weight_parameter/bn_param/running_var/{}'.format(7), mode='w') as f:\n",
    "                f.write(str(bn_model.running_var.data))\n",
    "            \n",
    "        self.start = self.start + num_b\n",
    "\n",
    "        conv_model.weight.data.copy_(\n",
    "            torch.reshape(torch.from_numpy(self.buf[self.start:self.start + num_w]), conv_model.weight.size()))\n",
    "        \n",
    "        \n",
    "        if conv_model.weight.data.shape == (16, 3, 3, 3):\n",
    "            scratch.params['W0'] = conv_model.weight.data\n",
    "            with open('./weight_parameter/conv_param/w/{}'.format(0), mode='w') as f:\n",
    "                f.write(str(conv_model.weight.data))\n",
    "        elif conv_model.weight.data.shape == (32, 16, 3, 3):\n",
    "            scratch.params['W1'] = conv_model.weight.data\n",
    "            with open('./weight_parameter/conv_param/w/{}'.format(1), mode='w') as f:\n",
    "                f.write(str(conv_model.weight.data))\n",
    "        elif conv_model.weight.data.shape == (64, 32, 3, 3):\n",
    "            scratch.params['W2'] = conv_model.weight.data\n",
    "            with open('./weight_parameter/conv_param/w/{}'.format(2), mode='w') as f:\n",
    "                f.write(str(conv_model.weight.data))\n",
    "        elif conv_model.weight.data.shape == (128, 64, 3, 3):\n",
    "            scratch.params['W3'] = conv_model.weight.data\n",
    "            with open('./weight_parameter/conv_param/w/{}'.format(3), mode='w') as f:\n",
    "                f.write(str(conv_model.weight.data))\n",
    "        elif conv_model.weight.data.shape == (256, 128, 3, 3):\n",
    "            scratch.params['W4'] = conv_model.weight.data\n",
    "            with open('./weight_parameter/conv_param/w/{}'.format(4), mode='w') as f:\n",
    "                f.write(str(conv_model.weight.data))\n",
    "        elif conv_model.weight.data.shape == (512, 256, 3, 3):\n",
    "            scratch.params['W5'] = conv_model.weight.data\n",
    "            with open('./weight_parameter/conv_param/w/{}'.format(5), mode='w') as f:\n",
    "                f.write(str(conv_model.weight.data))\n",
    "        elif conv_model.weight.data.shape == (1024, 512, 3, 3):            \n",
    "            scratch.params['W6'] = conv_model.weight.data\n",
    "            with open('./weight_parameter/conv_param/w/{}'.format(6), mode='w') as f:\n",
    "                f.write(str(conv_model.weight.data))\n",
    "        elif conv_model.weight.data.shape == (1024, 1024, 3, 3):\n",
    "            scratch.params['W7'] = conv_model.weight.data\n",
    "            with open('./weight_parameter/conv_param/w/{}'.format(7), mode='w') as f:\n",
    "                f.write(str(conv_model.weight.data))\n",
    "        self.start = self.start + num_w\n",
    "\n",
    "    def load_conv(self, conv_model):\n",
    "        num_w = conv_model.weight.numel()\n",
    "        num_b = conv_model.bias.numel()\n",
    "        conv_model.bias.data.copy_(\n",
    "            torch.reshape(torch.from_numpy(self.buf[self.start:self.start + num_b]), conv_model.bias.size()))\n",
    "        scratch.params['b8'] = conv_model.bias.data\n",
    "        with open('./weight_parameter/bias/{}'.format(7), mode='w') as f:\n",
    "            f.write(str(conv_model.bias.data))\n",
    "        self.start = self.start + num_b\n",
    "        conv_model.weight.data.copy_(\n",
    "            torch.reshape(torch.from_numpy(self.buf[self.start:self.start + num_w]), conv_model.weight.size()))\n",
    "        scratch.params['W8'] = conv_model.weight.data\n",
    "        with open('./weight_parameter/conv_param/w/{}'.format(8), mode='w') as f:\n",
    "            f.write(str(conv_model.weight.data))\n",
    "        self.start = self.start + num_w\n",
    "\n",
    "    def dfs(self, m):\n",
    "        children = list(m.children())\n",
    "        for i, c in enumerate(children):\n",
    "            if isinstance(c, torch.nn.Sequential):\n",
    "                self.dfs(c)\n",
    "            elif isinstance(c, torch.nn.Conv2d):\n",
    "                if c.bias is not None:\n",
    "                    self.load_conv(c)\n",
    "                else:\n",
    "                    self.load_conv_bn(c, children[i + 1])\n",
    "\n",
    "    def load(self, model, weights_file):\n",
    "        self.start = 0\n",
    "        fp = open(weights_file, 'rb')\n",
    "        header = np.fromfile(fp, count=4, dtype=np.int32)\n",
    "        self.buf = np.fromfile(fp, dtype=np.float32)\n",
    "        fp.close()\n",
    "        size = self.buf.size\n",
    "        self.dfs(model)\n",
    "        # make sure the loaded weight is right\n",
    "    \n",
    "        assert size == self.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weight_loader import WeightLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch = DeepConvNet(input_dims=(3, 416, 416),\n",
    "                        num_filters=[16, 32, 64, 128, 256, 512, 1024, 1024],\n",
    "                        max_pools=[0, 1, 2, 3, 4],\n",
    "                        weight_scale='kaiming',\n",
    "                        batchnorm=True,\n",
    "                        dtype=torch.float32, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Yolov2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightloader = WeightLoader()\n",
    "weightloader.load(model, './yolov2-tiny-voc.weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1, 3, 416, 416)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_scratch = scratch.loss(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_torch = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eecs598.grad.rel_error(output_scratch[0], output_torch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PythonConv = ConvB\n",
    "TorchConv = FastConvWB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 5, 5])\n",
      "Testing Conv.backward function\n",
      "dx error:  7.937759210108112e-17\n",
      "dw error:  8.086311976557575e-17\n",
      "db error:  1.643682260085236e-16\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "x = torch.randn(4, 3, 5, 5, dtype=torch.float64, device='cuda')\n",
    "w = torch.randn(2, 3, 3, 3, dtype=torch.float64, device='cuda')\n",
    "b = torch.randn(2, dtype=torch.float64, device='cuda')\n",
    "dout = torch.randn(4, 2, 5, 5, dtype=torch.float64, device='cuda')\n",
    "conv_param = {'stride': 1, 'pad': 1}\n",
    "\n",
    "dx_num = eecs598.grad.compute_numeric_gradient(lambda x: ConvB.forward(x, w, b, conv_param)[0], x, dout)\n",
    "dw_num = eecs598.grad.compute_numeric_gradient(lambda w: ConvB.forward(x, w, b, conv_param)[0], w, dout)\n",
    "db_num = eecs598.grad.compute_numeric_gradient(lambda b: ConvB.forward(x, w, b, conv_param)[0], b, dout)\n",
    "\n",
    "out, cache = ConvB.forward(x, w, b, conv_param)\n",
    "dx, dw, db = ConvB.backward(dout, cache)\n",
    "\n",
    "out2, cache2 = FastConvWB.forward(x, w, b, conv_param)\n",
    "dx2, dw2, db2 = FastConvWB.backward(dout, cache2)\n",
    "\n",
    "print('Testing Conv.backward function')\n",
    "print('dx error: ', eecs598.grad.rel_error(dx, dx2))\n",
    "print('dw error: ', eecs598.grad.rel_error(dw, dw2))\n",
    "print('db error: ', eecs598.grad.rel_error(db, db2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 5, 5])\n",
      "\n",
      "Testing Conv.forward function\n",
      "\tOut error:  1.4868675061790858e-16\n",
      "\n",
      "Testing Conv.backward function\n",
      "\tdx error:  7.937759210108112e-17\n",
      "\tdw error:  8.086311976557575e-17\n",
      "\tdb error:  1.643682260085236e-16\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "input \t\t\t= torch.randn(4, 3, 5, 5, dtype=torch.float64, device='cuda')\n",
    "weights\t\t\t= torch.randn(2, 3, 3, 3, dtype=torch.float64, device='cuda')\n",
    "bias \t\t\t= torch.randn(2, dtype=torch.float64, device='cuda')\n",
    "loss_grads\t\t= torch.randn(4, 2, 5, 5, dtype=torch.float64, device='cuda')\n",
    "conv_param \t\t= {'stride': 1, 'pad': 1}\n",
    "\n",
    "# Forward Propagation\n",
    "out, cache \t\t= PythonConv.forward(input, weights, bias, conv_param)\n",
    "out2, cache2 \t= TorchConv.forward( input, weights, bias, conv_param)\n",
    "\n",
    "# Loss Calculation\n",
    "# dummy loss gradient - generated randomly\n",
    "\n",
    "# Backward Propagation\n",
    "dx, dw, db \t\t= PythonConv.backward(dout, cache)\n",
    "dx2, dw2, db2 \t= TorchConv.backward(dout, cache2)\n",
    "\n",
    "# Check the output of Forward\n",
    "print('\\nTesting Conv.forward function')\n",
    "print('\\tOut error: ', eecs598.grad.rel_error(out, out2))\n",
    "\n",
    "# Check the output of Backward\n",
    "print('\\nTesting Conv.backward function')\n",
    "print('\\tdx error: ', eecs598.grad.rel_error(dx, dx2))\n",
    "print('\\tdw error: ', eecs598.grad.rel_error(dw, dw2))\n",
    "print('\\tdb error: ', eecs598.grad.rel_error(db, db2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 5, 5])\n",
      "torch.Size([2, 3, 3, 3])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(input.shape)\n",
    "print(weights.shape)\n",
    "print(bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del input, loss_grads, bias, weights\n",
    "loss_grads \t= pickle.load(open('../Temp_Files/Python/Backward_loss_gradients.pickle', 'rb'))\n",
    "input \t\t= pickle.load(open('../Temp_Files/Python/Forward_Out.pickle', 'rb'))[7]\n",
    "params  \t= pickle.load(open('../paramters.pickle', 'rb'))\n",
    "weights\t\t= params[\"W8\"]\n",
    "bias \t\t= params[\"b8\"]\n",
    "\n",
    "torch.manual_seed(42)\n",
    "input\t\t\t= input.to(device)\n",
    "weights\t\t\t= weights.to(device)\n",
    "bias \t\t\t= bias.to(device)\n",
    "loss_grads \t\t= loss_grads.to(device)\n",
    "conv_param \t\t= {'stride': 1, 'pad': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\t: torch.Size([1, 1024, 13, 13])\n",
      "weights\t: torch.Size([125, 1024, 1, 1])\n",
      "bias\t: torch.Size([125])\n",
      "input\t: cuda:0\n",
      "weights\t: cuda:0\n",
      "bias\t: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(f\"input\t: {input.shape}\")\n",
    "print(f\"weights\t: {weights.shape}\")\n",
    "print(f\"bias\t: {bias.shape}\")\n",
    "\n",
    "print(f\"input\t: {input.device}\")\n",
    "print(f\"weights\t: {weights.device}\")\n",
    "print(f\"bias\t: {bias.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Propagation\n",
    "out, cache \t\t= PythonConv.forward(input, weights, bias, conv_param)\n",
    "out2, cache2 \t= TorchConv.forward( input, weights, bias, conv_param)\n",
    "print(f\"{out[0,:10,0,0]}\\n\\n{out2[0,:10,0,0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.4574e-08, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Loss Calculation\n",
    "# Loss Gradient taken from code\n",
    "print(loss_grads[0,:10,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 13, 13])\n"
     ]
    }
   ],
   "source": [
    "# Backward Propagation\n",
    "dx, dw, db \t\t= PythonConv.backward(loss_grads, cache)\n",
    "dx2, dw2, db2 \t= TorchConv.backward (loss_grads, cache2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Conv.forward function\n",
      "\tOut error:  5.382756366892442e-07\n",
      "\n",
      "Testing Conv.backward function\n",
      "\tdx error:  1.0\n",
      "\tdw error:  1.0\n",
      "\tdb error:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Check the output of Forward\n",
    "print('\\nTesting Conv.forward function')\n",
    "print('\\tOut error: ', eecs598.grad.rel_error(out, out2))\n",
    "\n",
    "# Check the output of Backward\n",
    "print('\\nTesting Conv.backward function')\n",
    "print('\\tdx error: ', eecs598.grad.rel_error(dx, dx2))\n",
    "print('\\tdw error: ', eecs598.grad.rel_error(dw, dw2))\n",
    "print('\\tdb error: ', eecs598.grad.rel_error(db, db2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PythonLay0  = pickle.load(open('../Temp_Files/Python/Backward_loss_gradients.pickle', 'rb'))\n",
    "PytorchLay0 = pickle.load(open('../Temp_Files/Python/Backward_loss_gradients.pickle', 'rb'))\n",
    "print('\\tdb error: ', eecs598.grad.rel_error(db, db2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dx[0, 0, 0, :], dx2[0, 0, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "x = torch.rand(1, 1, 12, 12)\n",
    "w = torch.randn(3, 1, 3, 3)\n",
    "nn.init.ones_(w)\n",
    "conv_param = {'stride': 2, 'pad': 1}\n",
    "out, cache = Conv.forward(x, w, conv_param)\n",
    "# model = nn.Conv2d(1, 3, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "# if isinstance(model, nn.Conv2d):\n",
    "#     nn.init.ones_(model.weight)\n",
    "out2, cache2 = FastConv.forward(x, w, conv_param)\n",
    "eecs598.grad.rel_error(out, out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, w, conv_param = cache\n",
    "x2, w2, conv_param, tx, out2, layer = cache2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eecs598.grad.rel_error(x, x2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Conv_Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1, 1, 12, 12)\n",
    "w = torch.randn(3, 1, 3, 3)\n",
    "nn.init.ones_(w)\n",
    "conv_param = {'stride': 2, 'pad': 1}\n",
    "\n",
    "out, _ = Conv_ReLU.forward(x, w, conv_param)\n",
    "y = nn.Sequential(\n",
    "    nn.Conv2d(1, 3, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "    nn.LeakyReLU(0.1)\n",
    "    )\n",
    "for k in y.modules():\n",
    "    if isinstance(k, nn.Conv2d):\n",
    "        nn.init.ones_(k.weight)\n",
    "out2 = y(x)\n",
    "eecs598.grad.rel_error(out, out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1, 1, 12, 12)\n",
    "w = torch.randn(3, 1, 3, 3)\n",
    "nn.init.ones_(w)\n",
    "conv_param = {'stride': 2, 'pad': 1}\n",
    "pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "\n",
    "out, _ = Conv_ReLU_Pool.forward(x, w, conv_param, pool_param)\n",
    "y = nn.Sequential(\n",
    "    nn.Conv2d(1, 3, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "    nn.LeakyReLU(0.1),\n",
    "    nn.MaxPool2d(2, 2)\n",
    "    )\n",
    "for k in y.modules():\n",
    "    if isinstance(k, nn.Conv2d):\n",
    "        nn.init.ones_(k.weight)\n",
    "out2 = y(x)\n",
    "eecs598.grad.rel_error(out, out2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Conv_BatchNorm_ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1, 1, 12, 12)\n",
    "w = torch.randn(3, 1, 3, 3)\n",
    "nn.init.ones_(w)\n",
    "conv_param = {'stride': 2, 'pad': 1}\n",
    "pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "gamma = torch.ones(3)\n",
    "beta = torch.ones(3)\n",
    "bn_param = {'mode': 'train',\n",
    "            'running_mean': torch.ones(3),\n",
    "            'running_var': torch.ones(3)}\n",
    "\n",
    "out, _ = Conv_BatchNorm_ReLU.forward(x, w, gamma, beta, conv_param, bn_param)\n",
    "y = nn.Sequential(\n",
    "    nn.Conv2d(1, 3, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(3),\n",
    "    nn.LeakyReLU(0.1),\n",
    "    )\n",
    "for k in y.modules():\n",
    "    if isinstance(k, nn.Conv2d):\n",
    "        nn.init.ones_(k.weight)\n",
    "    if isinstance(k, nn.BatchNorm2d):\n",
    "        nn.init.ones_(k.weight)\n",
    "        nn.init.ones_(k.bias)\n",
    "out2 = y(x)\n",
    "eecs598.grad.rel_error(out, out2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Conv_BatchNorm_ReLU_Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1, 1, 12, 12)\n",
    "w = torch.randn(3, 1, 3, 3)\n",
    "nn.init.ones_(w)\n",
    "conv_param = {'stride': 2, 'pad': 1}\n",
    "pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "gamma = torch.ones(3)\n",
    "beta = torch.ones(3)\n",
    "bn_param = {'mode': 'train',\n",
    "            'running_mean': torch.ones(3),\n",
    "            'running_var': torch.ones(3)}\n",
    "\n",
    "out, _ = Conv_BatchNorm_ReLU_Pool.forward(x, w, gamma, beta, conv_param, bn_param, pool_param)\n",
    "y = nn.Sequential(\n",
    "    nn.Conv2d(1, 3, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(3),\n",
    "    nn.LeakyReLU(0.1),\n",
    "    nn.MaxPool2d(2, 2)\n",
    "    )\n",
    "for k in y.modules():\n",
    "    if isinstance(k, nn.Conv2d):\n",
    "        nn.init.ones_(k.weight)\n",
    "    if isinstance(k, nn.BatchNorm2d):\n",
    "        nn.init.ones_(k.weight)\n",
    "        nn.init.ones_(k.bias)\n",
    "out2 = y(x)\n",
    "eecs598.grad.rel_error(out, out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch11-6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
